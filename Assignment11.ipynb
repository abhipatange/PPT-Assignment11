{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e13d7-8807-4399-8573-ee7cc73747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How do word embeddings capture semantic meaning in text preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea394418-b6a1-4b87-8149-03e63e8a2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors\n",
    "    in a lower-dimensional space and captures inter-word semantics.\n",
    "    Each word is represented by a real-valued vector with tens or hundreds of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fbf05-ad85-49e1-9ca8-6ed9c9d0d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea77ce-9ce6-44c7-810b-3540e6214cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the concept of recurrent neural networks (RNNs) and their role in text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd0a1d-ccb0-48cb-b5f8-872cbe5a38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    A recurrent neural network is a type of artificial neural network commonly\n",
    "    used in speech recognition and natural language processing.\n",
    "    Recurrent neural networks recognize datas sequential characteristics \n",
    "    and use patterns to predict the next likely scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997bc1a-6dbf-4515-86b8-2c1fbd314aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69def4-a7a3-4354-9979-20617388371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is the encoder-decoder concept, and how is it applied in tasks like machine translation or text summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e3302-655d-434f-8f91-f8ecbf1a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Encoder-decoder architecture is used in machine translation, text summarization, and image captioning tasks.\n",
    "      Encoder compresses input to fixed-length vector; decoder generates output from it. \n",
    "        It can be implemented using RNNs or transformer networks and trained using input-output pairs to learn to map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b6b40-5aa0-48fe-98c9-730e1dbd3b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1c78c-aff6-4d88-9404-a0b7f60046bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Discuss the advantages of attention-based mechanisms in text processing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81ecb6-9bc7-4d9f-8737-fc40105db70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     The attention mechanism allows the model to \"pay attention\" to certain parts of the \n",
    "     data and to give them more weight when making predictions. \n",
    "    In a nutshell, the attention mechanism helps preserve the context of every word \n",
    "     in a sentence by assigning an attention weight relative to all other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f8147-e84c-4fff-9327-92a6e51c60e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d7af8-a7cc-4738-99ad-073c62fd9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain the concept of self-attention mechanism and its advantages in natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7644c0c-13e7-4d00-8a74-68026ed21c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Self-Attention. The attention mechanism allows output to focus attention on input while producing\n",
    "    output while the self-attention model allows inputs to interact with each other \n",
    "    i.e calculate attention of all other inputs wrt one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09621a06-fced-4a47-b894-2d2b91f0adb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017582a4-31b3-4c7b-b04c-ab5ed44497f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is the transformer architecture, and how does it improve upon traditional RNN-based models in text processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503e9a0-15de-4990-8177-0d1cd31763d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Recurrent neural networks (RNNs) are a type of neural network architecture designed to handle sequential data, \n",
    "     making them well-suited for text processing tasks.\n",
    "    RNNs maintain an internal memory state that enables them to capture dependencies between words or elements in a sequence.\n",
    "    They process input step-by-step, updating their hidden state at each step based on the current input and the previous hidden state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faccc4d1-bd98-40f1-9739-a2663d48b238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40ac30-96e4-4bd8-a5ae-a9cf5dd2b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Describe the process of text generation using generative-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881a7f9-cd8b-437d-a056-41cd00c18429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Text Generation is the task of generating text with the goal of appearing indistinguishable to human-written text. \n",
    "     This task if more formally known as \"natural language generation\" in the literature.\n",
    "\n",
    "    Text generation can be addressed with Markov processes or deep generative models like LSTMs.\n",
    "    Recently, some of the most advanced methods for text generation include BART, GPT and other GAN-based approaches.\n",
    "    Text generation systems are evaluated either through human ratings or automatic evaluation metrics like METEOR, ROUGE, and BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24924807-919f-4f9e-9d26-765f16c4f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb2118-09da-4553-96c3-383cf4710d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are some applications of generative-based approaches in text processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6ee4d-03c9-4e34-a5c4-39a07ab1162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    Potential applications: Data augmentation, dataset synthesis, art creation,\n",
    "    code generation, text generation, audio synthesis, etc.\n",
    "    Synthesized by score-based generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c35dcf-7373-44e9-9013-460456f9b10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccf001-5504-4d3e-b1e6-1cbf1c14a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Discuss the challenges and techniques involved in building conversation AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e214e1-ce27-4ae6-a2fd-90e8d8235ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Conversation AI refers to the application of artificial intelligence techniques in building chatbots \n",
    "    or virtual assistants capable of engaging in human-like conversations. \n",
    "    It involves understanding and generating natural language responses, maintaining context and coherence, \n",
    "    and providing relevant and helpful information to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e43eb2-85ef-4981-aee0-a08b36b04e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf9248-0485-41a1-91f0-618e04e7064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. How do you handle dialogue context and maintain coherence in conversation AI models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f1ac0-2ca6-411e-bc7a-860fdd6d5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "    1. Context tracking: Keeping track of the conversation history, including user queries and system responses, \n",
    "     to maintain a consistent understanding of the dialogue context.\n",
    "\n",
    "    2. Coreference resolution: Resolving pronouns or references to entities mentioned earlier in the conversation to avoid ambiguity.\n",
    "\n",
    "    3. Dialogue state management: Maintaining a structured representation of the dialogue state, including user intents,\n",
    "      slots, and system actions, to guide the conversation flow.\n",
    "\n",
    "    4. Coherent response generation: Generating responses that are coherent with the dialogue context and align with \n",
    "       the user s intent and expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9645bb6-cbc7-464f-bbd4-c0e8c4b36107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a749a3-350c-48bf-bfca-d1c2602ecf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Explain the concept of intent recognition in the context of conversation AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab21caa-f5d1-4732-9fbe-3b042d346581",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "      Intent recognition in conversation AI involves identifying the underlying intent or purpose behind user queries or statements.\n",
    "      It helps understand what the user wants to achieve and guides the systems response. \n",
    "        Techniques for intent recognition include rule-based approaches, machine learning classifiers,\n",
    "        or deep learning models like recurrent neural networks (RNNs) or transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbfa5a-8807-4fa5-80e1-640721933f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b01f4f-3cd4-4a6f-88f1-53c08f9353d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Discuss the advantages of using word embeddings in text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208ce68-d6e0-45e4-8af1-13450f75614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS= \n",
    "    Word Embeddings help us understand the meaning of each word, which can be used to recommend articles,\n",
    "    suggest automations, and enable more features based on the dialogue meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4cce8-f9f9-45dc-91d3-e6b8a356fd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41be5c-af42-4c02-a8ca-012c1ce42eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. How do RNN-based techniques handle sequential information in text processing tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af499b4e-257a-4463-94a7-0ec81c662a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     1. Parallelism: The transformer model allows for parallel processing of input sequences, enabling faster training \n",
    "         and inference compared to sequential processing in RNNs.\n",
    "\n",
    "     2. Capturing long-range dependencies: The self-attention mechanism in transformers enables the model to capture\n",
    "         long-range dependencies more effectively compared to the limited context captured by RNNs.\n",
    "\n",
    "     3. Handling variable-length sequences: RNNs require fixed-length hidden states, which can be problematic for \n",
    "         tasks with variable-length input sequences.\n",
    "           Transformers handle variable-length sequences naturally through self-attention, making them more flexible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b000c3b-ce48-45a6-ac8e-4cff4361ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd2c70-7c76-45e7-b376-bf389b0b72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. What is the role of the encoder in the encoder-decoder architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b64e4-bf42-4050-a99a-c3c83ad24628",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "     Encoder-decoder architectures can handle inputs and outputs that both consist of variable-length\n",
    "       sequences and thus are suitable for seq2seq problems such as machine translation. \n",
    "        The encoder takes a variable-length sequence as input and transforms it into a state with a fixed shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e51015-b3f4-4055-920f-d270065f5260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35ab0b-9780-486f-b24e-a21ea05820f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Explain the concept of attention-based mechanism and its significance in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ac81d-3cc5-4f30-a4ea-1ca4abda8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "    In machine translation, attention mechanism is used to align and selectively focus on relevant parts\n",
    "    of the source sentence during the translation process.\n",
    "    It allows the model to assign weights to more important words or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a69d7d-3563-4626-9c51-5ebbe0b25257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67015db3-6238-4307-9c1b-0d802c695709",
   "metadata": {},
   "outputs": [],
   "source": [
    "16. How does self-attention mechanism capture dependencies between words in a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c685a3-685b-41e0-a4d3-f10127d4b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    Self-attention mechanism:\n",
    "\n",
    "    The attention mechanism allows output to focus attention on input while producing output while the\n",
    "    self-attention model allows inputs to interact with each other i.e calculate attention of all other inputs wrt one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07173c02-59b8-4a1d-9c33-f149265191b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fea22-f24f-48ec-b773-ad813c334b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "17. Discuss the advantages of the transformer architecture over traditional RNN-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710c569-fff7-42e0-ad9b-4fe60b925547",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    As discussed, transformers are faster than RNN-based models as all the input is ingested once.\n",
    "    Training LSTMs is harder when compared with transformer networks, since the number \n",
    "    of parameters is a lot more in LSTM networks.\n",
    "    Moreover, its impossible to do transfer learning in LSTM networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295968d-9e5e-4582-ae3b-19319b3a8357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850c632-d483-4a47-b26c-fee706190e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. What are some applications of text generation using generative-based approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c759dac-bb29-4b15-b2ca-dc757a24ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS= \n",
    "    Popular applications of text generation in NLP\n",
    "    One popular application of text generation is machine translation,\n",
    "    where the model is trained to translate text from one language to another. \n",
    "    Another application is content creation, where the model can generate articles, \n",
    "    summaries, or social media posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2693bf-e56c-4880-a2a0-b3cb060712ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bf93a-37ba-43ae-80e0-7bcd0d744698",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. How can generative models be applied in conversation AI systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ee6c9-6d78-4b38-b0e7-aeb786a2202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Generative AI models can engage in human-like conversations with users. \n",
    "     They can understand natural language inputs and generate relevant and contextually appropriate responses.\n",
    "    This makes them valuable for applications such as customer support chatbots, \n",
    "     virtual assistants, and dialogue systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4d3de-fb67-4d56-a993-f21cbc3357c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd34f1-4aeb-4461-a45b-d4b54f2c2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "20. Explain the concept of natural language understanding (NLU) in the context of conversation AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c81546-40a3-48c1-9d2e-5d515bbc1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "     NLU enables human-computer interaction.\n",
    "     It is the comprehension of human language such as English, Spanish and French,\n",
    "        for example, that allows computers to understand commands without the formalized syntax of computer languages. \n",
    "        NLU also enables computers to communicate back to humans in their own languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f92058-5efa-4dcb-a9ea-151c0b1f269a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ee87f-3970-439c-b43e-633329bdae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. What are some challenges in building conversation AI systems for different languages or domains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67f8ff-c181-4ace-8de6-c3f193600af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "    Some common challenges faced in the development and deployment of conversational AI solutions \n",
    "    include maintaining context and understanding nuances in conversations, handling highly \n",
    "    specialized knowledge or domain-specific questions, and ensuring data privacy and security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521d24e-2d69-4a99-a67e-5643ec26f963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbefd9-e81e-469b-abe1-0458925beacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. Discuss the role of word embeddings in sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea5b58-91fc-41a6-a8df-ee28418a0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "    In sentiment analysis, the goal is to classify text as having positive, negative, or neutral sentiment. \n",
    "    Word embeddings can be used to represent the words in the text being analyzed and these embeddings \n",
    "    can then be used as input to a machine learning model that is trained to classify the sentiment of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba671a-fdda-4ec8-9d15-6d6fd78aac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f7721-ebe5-4d47-9161-1d85a66c5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. How do RNN-based techniques handle long-term dependencies in text processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f7d15-f97d-4b98-b2ec-96dfd119a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    In RNNs are absolutely capable of handling such “long-term dependencies.” \n",
    "    A human could carefully pick parameters for them to solve toy problems of this form. \n",
    "    Sadly, in practice, recurrent neural network don not seem to be able to learn them. \n",
    "    This problem is called Vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84389a8-d2d1-41ce-9ce2-30cbf91e6b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadd2bf-fc3d-40ba-8234-69dc274a3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. Explain the concept of sequence-to-sequence models in text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404189b-a75e-48cd-85d1-be65d59d929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Sequence-to-Sequence is a type of model in machine learning that is used for tasks such as machine translation,\n",
    "    text summarization, and image captioning.\n",
    "    The model consists of two main components: Encoder. Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67ee8e-308a-4fa0-9506-875a49d53475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cecdb-e2e1-447a-8ed5-76c0054df87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. What is the significance of attention-based mechanisms in machine translation tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068f97-9317-462d-96e4-3d4a2c7c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     In machine translation, attention mechanism is used to align and selectively focus on relevant\n",
    "      parts of the source sentence during the translation process. \n",
    "        It allows the model to assign weights to more important words or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47eaa44-5a17-4533-a13b-cd9c4e25abc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3b121-ffb0-41c2-b6ce-357e444c8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. Discuss the challenges and techniques involved in training generative-based models for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2dbee-d1d9-4d68-905a-0c29a8974457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "     Generative AI utilizes deep learning, neural networks, and machine learning techniques to enable \n",
    "      computers to produce content that closely resembles human-created output autonomously. \n",
    "        These algorithms learn from patterns, trends, and relationships within the training data to\n",
    "        generate coherent and meaningful content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dfc43-0604-4d50-80c5-3c2b85f70247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c284c7-bb4a-4fdf-a90f-9c460a7fb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. How can conversation AI systems be evaluated for their performance and effectiveness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664e252-11d8-4270-915c-27d01e728206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Conversational AI works by combining natural language processing (NLP) and machine learning (ML)\n",
    "     processes with conventional, static forms of interactive technology, such as chatbots.\n",
    "        This combination is used to respond to users through interactions that mimic those\n",
    "        with typical human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9933a0-aeef-4a0a-986e-97bf888336be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17b75a-87d8-4c1f-bbff-30c83fbc97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. Explain the concept of transfer learning in the context of text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ab180-ccd1-44e4-b611-77792b33fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    Transfer learning is a technique where a deep learning model trained on a large dataset is \n",
    "    used to perform similar tasks on another dataset. \n",
    "    We call such a deep learning model a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776396e-2945-42b0-9f79-f4d455a338a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f69304-1eda-49f7-8608-5c10817c5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. What are some challenges in implementing attention-based mechanisms in text processing models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c6f94-03fe-4ff6-869b-f1cfa2a4aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     One of the main drawbacks of this network is its inability to extract strong contextual relations\n",
    "      from long semantic sentences, that is if a particular piece of long text has some context or \n",
    "        relations within its substrings, then a basic seq2seq model[ short form for sequence to sequence]\n",
    "        cannot identify those contexts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e0afc-0b1a-4919-a661-3c60bbab18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4858a-c43d-4734-9157-6a39e95f4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. Discuss the role of conversation AI in enhancing user experiences and interactions on social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa6b80-f79e-4199-af77-d7d210e09c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Conversational AI is transforming the way brands interact with their customers.\n",
    "      By providing personalized, 24/7 support, improving efficiency, reducing costs\n",
    "        and providing valuable customer insights, conversational AI is enhancing the customer e\n",
    "        xperience and driving customer loyalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ca0de-52d1-4b4a-8123-81b150b19fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0803925-fe1e-415d-8295-c560cfbccd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
